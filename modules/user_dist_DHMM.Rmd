---
title: "User-defined distribution example: Capture-recapture with dDHMM"
subtitle: "NIMBLE training materials module"
author: "NIMBLE Development Team"
output:
  html_document:
    code_folding: show
---

# Multi-state capture-recapture example

We will use the orchid (Showy Lady's Slipper) example from Chapter 9 of K&eacute;ry and Schaub (2012)[^1]

We will show code but not take the time to run as a working example.

[^1]: 
Marc K&eacute;ry and Michael Schaub. 2012. Bayesian Population Analysis Using WinBUGS: A hierarchical perspective.  Elsevier / Academic Press.


    - States are vegetative (1), flowering (2), dormant (3), or dead (4)
    - Observations are "seen vegetative" (1), "seen flowering" (2), or "not seen" (3).
    - Data from 250 plants on 11 sampling occassions.
    - Capture histories look like:
        + 1 1 2 2 2 2 1 2 2 2 2
        + 1 1 0 1 1 1 0 0 0 0 0
        + 0 0 0 1 2 2 1 0 0 2 2
    - Observations match state if vegetative (1) or flowering (2) and are "not seen" when dormant (3) or dead (4).
    - Although simple, this has the basic math of uncertain states.
    - Parameters are survival and probabilities of transitions between states.
    - Typical BUGS implementation uses discrete latent state which must be sampled by MCMC.
    - For one individual, probability of capture history can be calculated from a Dynamic Hidden Markov Model.
    - In NIMBLE we can replace the latent states by a distribution for each capture history.

# Typical BUGS implementation

Taken from K\&S 9.7

```{r eval = FALSE}
{

# -------------------------------------------------
# Parameters:
# s: survival probability
# psiV: transitions from vegetative
# psiF: transitions from flowering
# psiD: transitions from dormant
# -------------------------------------------------
# States (S):
# 1 vegetative
# 2 flowering
# 3 dormant
# 4 dead
# Observations (O):  
# 1 seen vegetative 
# 2 seen flowering
# 3 not seen
# -------------------------------------------------

# Priors and constraints
   # Survival: uniform
   for (t in 1:(n.occasions-1)){  
      s[t] ~ dunif(0, 1)
      }
   # Transitions: gamma priors
   for (i in 1:3){
      a[i] ~ dgamma(1, 1)
      psiD[i] <- a[i]/sum(a[])
      b[i] ~ dgamma(1, 1)
      psiV[i] <- b[i]/sum(b[])
      c[i] ~ dgamma(1, 1)
      psiF[i] <- c[i]/sum(c[])
      }

# Define state-transition and observation matrices 	
for (i in 1:nind){
   # Define probabilities of state S(t+1) given S(t)
   for (t in 1:(n.occasions-1)){
      ps[1,i,t,1] <- s[t] * psiV[1]
      ps[1,i,t,2] <- s[t] * psiV[2]
      ps[1,i,t,3] <- s[t] * psiV[3]
      ps[1,i,t,4] <- 1-s[t]
      ps[2,i,t,1] <- s[t] * psiF[1]
      ps[2,i,t,2] <- s[t] * psiF[2]
      ps[2,i,t,3] <- s[t] * psiF[3]
      ps[2,i,t,4] <- 1-s[t]
      ps[3,i,t,1] <- s[t] * psiD[1]
      ps[3,i,t,2] <- s[t] * psiD[2]
      ps[3,i,t,3] <- s[t] * psiD[3]
      ps[3,i,t,4] <- 1-s[t]
      ps[4,i,t,1] <- 0
      ps[4,i,t,2] <- 0
      ps[4,i,t,3] <- 0
      ps[4,i,t,4] <- 1

      # Define probabilities of O(t) given S(t)
      po[1,i,t,1] <- 1
      po[1,i,t,2] <- 0
      po[1,i,t,3] <- 0
      po[2,i,t,1] <- 0
      po[2,i,t,2] <- 1
      po[2,i,t,3] <- 0
      po[3,i,t,1] <- 0
      po[3,i,t,2] <- 0
      po[3,i,t,3] <- 1
      po[4,i,t,1] <- 0
      po[4,i,t,2] <- 0
      po[4,i,t,3] <- 1
      } #t
   } #i

# Likelihood 
for (i in 1:nind){
   # Define latent state at first capture
   z[i,f[i]] <- y[i,f[i]]
   for (t in (f[i]+1):n.occasions){
      # State process: draw S(t) given S(t-1)
      z[i,t] ~ dcat(ps[z[i,t-1], i, t-1,])
      # Observation process: draw O(t) given S(t)
      y[i,t] ~ dcat(po[z[i,t], i, t-1,])
      } #t
   } #i
}
```

Key element is that `z[i,t]` are latent states (true state of plant i at time t) and hence need MCMC sampling.  But in this case many of the states are known with certainty, but we're using it because it is simple.

Actually this code will not run in NIMBLE because it uses stochastic indices.  We expect a future version of NIMBLE to support that.

# NIMBLE implementation using a hidden-Markov model distribution.

This is from [Turek et al.](http://arxiv.org/abs/1601.02698), recently accepted.

```{r eval=FALSE}
## Filter MCMC for the orchid model

## load nimble library
library(nimble)

## define custom distribution
dDHMMorchid <- nimbleFunction(
    run = function(x = double(1), length = double(),
    	  	   prior = double(1), Z = double(2),
		   T = double(3), log.p = double()) {
        pi <- prior
        logL <- 0
        for(t in 1:length) {
            Zpi <- Z[x[t], ] * pi
            sumZpi <- sum(Zpi)
            logL <- logL + log(sumZpi)
            if(t != length)
	       pi <- (T[,,t] %*% asCol(Zpi) / sumZpi)[ ,1]
        }
        returnType(double())
        return(logL)
    }
)

rDHMMorchid <- nimbleFunction(
    run = function(n = integer(), length = double(),
                   prior = double(1), Z = double(2),
		   T = double(3)) {
        declare(x, double(1, length))
        returnType(double(1))
        return(x)
    }
)

registerDistributions(list(
    dDHMMorchid = list(
        BUGSdist = 'dDHMMorchid(length, prior, Z, T)',
        types = c('value = double(1)', 'length = double()', 'prior = double(1)', 'Z = double(2)', 'T = double(3)'),
        discrete = TRUE
    )
))

## load data
load('../data/orchidData.RData')

## define hierarchical model
code <- nimbleCode({
    for (t in 1:(k-1)) {
        s[t] ~ dunif(0, 1)
    }
    for (i in 1:3) {
        a[i] ~ dgamma(1, 1) 
        psiD[i] <- a[i]/sum(a[1:3]) 
        b[i] ~ dgamma(1, 1) 
        psiV[i] <- b[i]/sum(b[1:3]) 
        c[i] ~ dgamma(1, 1) 
        psiF[i] <- c[i]/sum(c[1:3]) 
    }
    for (t in 1:(k-1)) {
        T[1,1,t] <- s[t] * psiV[1]
        T[2,1,t] <- s[t] * psiV[2]
        T[3,1,t] <- s[t] * psiV[3]
        T[4,1,t] <- 1-s[t]
        T[1,2,t] <- s[t] * psiF[1]
        T[2,2,t] <- s[t] * psiF[2]
        T[3,2,t] <- s[t] * psiF[3]
        T[4,2,t] <- 1-s[t]
        T[1,3,t] <- s[t] * psiD[1]
        T[2,3,t] <- s[t] * psiD[2]
        T[3,3,t] <- s[t] * psiD[3]
        T[4,3,t] <- 1-s[t]
        T[1,4,t] <- 0
        T[2,4,t] <- 0
        T[3,4,t] <- 0
        T[4,4,t] <- 1
    }
    T[1,1,k] <- 1
    T[2,1,k] <- 0
    T[3,1,k] <- 0
    T[4,1,k] <- 0
    T[1,2,k] <- 0
    T[2,2,k] <- 1
    T[3,2,k] <- 0
    T[4,2,k] <- 0
    T[1,3,k] <- 0
    T[2,3,k] <- 0
    T[3,3,k] <- 1
    T[4,3,k] <- 0
    T[1,4,k] <- 0
    T[2,4,k] <- 0
    T[3,4,k] <- 0
    T[4,4,k] <- 1
    for (i in 1:nind) {
        y[i, f[i]:k] ~ dDHMMorchid(length = k-f[i]+1,
	                           prior = prior[1:4],
				   Z = Z[1:3,1:4],
				   T = T[1:4,1:4,f[i]:k])
    }
})

## define model constants, data, and initial values
constants <- list(f=f, k=k, nind=nind, prior=c(1/2,1/2,0,0), Z=array(c(1,0,0,0,1,0,0,0,1,0,0,1), c(3,4)))
data <- list(y = y)
inits <- list(s = rep(1/2,k-1), a = rep(1,3), b = rep(1,3), c = rep(1,3))

## create model object
Rmodel <- nimbleModel(code, constants, data, inits, check = FALSE)

## specify MCMC algorithm
spec <- configureMCMC(Rmodel)
spec$printSamplers()

## build MCMC algorithm
Rmcmc <- buildMCMC(spec)

## compile model and MCMC
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Rmodel)

## run MCMC
set.seed(0)
Cmcmc$run(10000)

## extract samples
samples <- as.matrix(Cmcmc$mvSamples)
apply(samples, 2, mean)
```

Key element is that there are no latent states. `dDHMM` sums over the latent states using a Hidden Markov Model filtering calculation for each capture history.

# Results 


In this case the dDHMM by itself did not yield more efficient MCMC than JAGS.

This may be because many of the states are known from perfect observations and hence do not need to be sampled in JAGS.

Combining dDHMM with *parameter blocking* yielded about 3x more efficiency (minimum ESS/time) than JAGS.

For Goose example (> 11000 animals), taking advantage also of multiple identical capture histories (only 153 unique capture histories), dDHMM with default samplers yielded about 70x improvement over JAGS.  Parameter blocking yielded another order of magnitude improvement.

