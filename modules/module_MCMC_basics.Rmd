Basics of MCMC in Nimble
==========================
NIMBLE training materials module
---------------------------------
Nimble Development Team

```{r chunksetup, include=FALSE} 
# include any code here you don't want to show up in the document,
# e.g. package and dataset loading
if(!('modules' %in% unlist(strsplit(getwd(), split = '/')))) setwd('modules')
library(methods)  # otherwise new() not being found - weird
library(nimble)
```

# Setting up the pump model

Here's the graph of the pump model discussed in the BUGS_models module.

<center><img src="figures/pumpDAG.jpg"></center>

These steps just repeat what we did in the BUGS_model module.

```{r, model-setup}
library(nimble)
pumpCode <- nimbleCode({ 
  for (i in 1:N){
      theta[i] ~ dgamma(alpha,beta)
      lambda[i] <- theta[i]*t[i]
      x[i] ~ dpois(lambda[i])
  }
  alpha ~ dexp(1.0)
  beta ~ dgamma(0.1,1.0)
})

N <- 10
t <- c(94.3, 15.7, 62.9, 126, 5.24, 31.4, 1.05, 1.05, 2.1, 10.5)
x <- c(5, 1, 5, 14, 3, 19, 1, 1, 4, 22)
pumpConsts <- list(t = t, N = 10)
pumpData <- list(x = x)
pumpInits <- list(alpha = 1, beta = 1,
         theta = rep(0.1, pumpConsts$N))
pump <- nimbleModel(pumpCode, 
          data = pumpData, constants = pumpConsts, inits = pumpInits)
Cpump <- compileNimble(pump)
```

# Configuring a basic MCMC

Setting up and running an MCMC in Nimble takes a few more steps than in BUGS or JAGS, but with the benefit of giving the user much more control of how the MCMC operates.

First we *configure* the MCMC, which means setting up the samplers to be used for each node or group of nodes. Nimble provides a default configuration, but we'll see shortly how you can modify that. 

```{r, conf}
pumpConf <- configureMCMC(pump, print = TRUE)
```
You also specify what nodes you'd like to get the MCMC samples provided as output.

```{r, monitor}
pumpConf$addMonitors(c('alpha', 'beta', 'theta'))
```

# Building the MCMC algorithm for the model 

Next we'll build the MCMC algorithm for the model under the default configuration. And we'll create a compiled (i.e., C++) version of the MCMC that is equivalent in functionality but will run much faster.

```{r build-mcmc}
pumpMCMC <- buildMCMC(pumpConf)
CpumpMCMC <- compileNimble(pumpMCMC, project = pump)
```

# Running the MCMC

Now let's run the MCMC. We don't recommend running the R version of the MCMC for very many iterations - it's really slow - in part because iterating in R is slow and in part because iterating with a model in Nimble requires even more overhead. 

```{r run-mcmc}
niter <- 1000
set.seed(0)
print(system.time(pumpMCMC$run(5)))  # R version
set.seed(0)
print(system.time(
CpumpMCMC$run(niter)
))
```

# Working with MCMC output

The R and C MCMC samples are the same, so you can use the R MCMC for debugging. It's possible to step through the code line by line using R's debugging capabilities (not shown).

```{r Rmcmc}
Rsamples <- as.matrix(pumpMCMC$mvSamples)
samples <- as.matrix(CpumpMCMC$mvSamples)

identical(Rsamples, samples[1:5, ])
```

Now let's look at the MCMC performance

```{r output-mcmc, fig.height=5, fig.width=12}
tsplot <- function(x, ...) plot(seq_along(x), x, type = 'l', ...)

par(mfrow = c(1, 4), mai = c(.6, .5, .1, .2))
tsplot(samples[ , 'alpha'], xlab = 'iteration',
     ylab = expression(alpha), main = expression(alpha))
tsplot(samples[ , 'beta'], xlab = 'iteration',
     ylab = expression(beta), main = expression(beta))
plot(samples[ , 'alpha'], samples[ , 'beta'], xlab = expression(alpha),
     ylab = expression(beta), main = paste(expression(alpha), expression(beta), "dependence"))
tsplot(samples[ , 'theta[1]'], xlab = 'iteration',
     ylab = expression(theta[1]), main = expression(theta[1]))
```

# Using CODA

Nimble does not provide any MCMC diagnostics. (At least not yet; there's no reason one couldn't write code for various diagnostics using the Nimble system.)  But one can easily use CODA or other R packages with the MCMC output from a Nimble MCMC.

```{r coda}
library(coda)
burnin <- 100
mcmc <- as.mcmc(samples[(burnin+1):nrow(samples), ])
crosscorr(mcmc[ , c('alpha', 'beta', 'theta[1]', 'theta[2]', 'theta[3]')])
effectiveSize(mcmc)
```

One could apply the commonly used Gelman-Rubin potential scale reduction factor diagnostic, but one would need to run multiple chains, as will be discussed shortly.


# Customizing samplers

One of Nimble's most important features is that users can easily modify the MCMC algorithm used for their model. The easiest thing to do is to start with Nimble's default MCMC and then make modifications. 

```{r customize-mcmc}
pumpConf$getSamplers()
pumpConf$removeSamplers('alpha')
pumpConf$addSampler(target = c('alpha'), type = 'slice')

pumpMCMC <- buildMCMC(pumpConf)
CpumpMCMC <- compileNimble(pumpMCMC, project = pump, resetFunctions = TRUE)

Cpump$setInits(pumpInits)
set.seed(0)
CpumpMCMC$run(niter)
```

We can look at diagnostics and see if the change in samplers had an effect. Interestingly, despite the posterior correlation between $\alpha$ and $\beta$, a simple change just to the univariate sampler for $\alpha$ has had a real effect on MCMC performance.

```{r output2, fig.height=5, fig.width=12}
samples2 <- as.matrix(CpumpMCMC$mvSamples)
mcmc2 <- as.mcmc(samples2[(burnin+1):nrow(samples2), ])
crosscorr(mcmc2[ , c('alpha', 'beta', 'theta[1]', 'theta[2]', 'theta[3]')])
effectiveSize(mcmc2)

par(mfrow = c(1, 4), mai = c(.6, .5, .1, .2))
plot(samples2[ , 'alpha'], type = 'l', xlab = 'iteration',
     ylab = expression(alpha), main = expression(alpha))
plot(samples2[ , 'beta'], type = 'l', xlab = 'iteration',
     ylab = expression(beta), main = expression(beta))
plot(samples2[ , 'alpha'], samples[ , 'beta'], xlab = expression(alpha),
     ylab = expression(beta), main = paste(expression(alpha), expression(beta), "dependence"))
plot(samples2[ , 'theta[1]'], type = 'l', xlab = 'iteration',
     ylab = expression(theta[1]), main = expression(theta[1]))
```

# Blocking parameters

Often a key factor that reduces MCMC performance is dependence between parameters that limits the ability of univariate samplers to move very far. A standard strategy is to sample correlated parameters in blocks. Unlike many other MCMC engines, Nimble makes it easy for users to choose what parameters to sample in blocks.

We'll try that here for $\alpha$ and $\beta$.


```{r customize-mcmc2}
pumpConf$getSamplers()
pumpConf$removeSamplers(c('alpha', 'beta'))
pumpConf$addSampler(target = c('alpha','beta'), type = 'RW_block', 
control = list(adaptInterval = 100))

pumpMCMC <- buildMCMC(pumpConf)
CpumpMCMC <- compileNimble(pumpMCMC, project = pump, resetFunctions = TRUE)

Cpump$setInits(pumpInits)
set.seed(0)
CpumpMCMC$run(niter)

samples3 <- as.matrix(CpumpMCMC$mvSamples)
mcmc3 <- as.mcmc(samples3[(burnin+1):nrow(samples3), ])
crosscorr(mcmc3[ , c('alpha', 'beta', 'theta[1]', 'theta[2]', 'theta[3]')])
effectiveSize(mcmc3)
```

In this case the block sampler appears to be less effective, but it might just be that the adaptation hasn't had enough time to take full effect in only 1000 iterations.  Very often block sampling gives big improvements.

# Running multiple chains

At the moment, we haven't set up Nimble to automatically run multiple chains, such as would be needed for calculation of the Gelman-Rubin potential scale reduction factor (*gelman.diag()* in the *coda* package). But it's straightforward to do this yourself.

```{r, multi-chain}
nChains <- 3
pumpInits <- list(list(alpha = 1, beta = 1), list(alpha = 0.1, beta =30), list(alpha = 30, beta = 0.1))
out <- list()
set.seed(0)
niter <- 3000
burnIn <- 1000
for(ch in seq_len(nChains)) {
       Cpump$setInits(pumpInits[[ch]])
       Cpump$simulate(pump$getDependencies(c('alpha','beta')))
       Cmcmc <- compileNimble(pumpMCMC, project = pump, resetFunctions = TRUE)
       Cmcmc$run(niter)
       out[[ch]] <- as.mcmc(as.matrix(Cmcmc$mvSamples)[(1+burnIn):niter, ])
}
```

Considerations: you'll want to think about how to set up the over-dispersed starting points and the number of iterations to use for burn-in.


CHECK if can use a single chain

# Assessing MCMC performance from multiple chains

```{r, gelman-rubin}
smp <- do.call(mcmc.list, out)
gelman.diag(smp)
# and here's a graphical representation of the information
tsplot(out[[1]][ , 'alpha'], xlab = 'iteration',
     ylab = expression(alpha), main = expression(alpha))
sq <- seq_along(out[[1]][ , 'alpha'])
for(i in 2:3)
      lines(sq, out[[i]][ , 'alpha'], col = i)
```



# Exercises

1) Compare the MCMC performance of using univariate slice samplers on $\alpha$ and $\beta$ to the default samplers and to the blocked Metropolis sampler for ${\alpha, \beta}$ for longer MCMC runs. 

2) Change the sampler for $\theta$ to be a single block Metropolis sampler. How does that affect MCMC performance? How much time does it take to run a set of 10 individual conjugate samplers on the elements of $\theta$ (from the default configuration) compared to a single block sampler on the vector $\theta$. 

3) Suppose you wanted to consider the predictive distribution for a new pump. 
    - Create a new version of the model that allows for this.  
    - Set the data for the new model such that the new pump's number of failures is missing and not treated as a data node (see the BUGS_model module for information on working with data).
    - Set up an MCMC for the new model. What kind of sampling should happen for the new pump failures? 
