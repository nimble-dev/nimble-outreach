---
title: "Running MCEM for likelihood maximization"
subtitle: "NIMBLE training materials module"
author: "NIMBLE Development Team"
output:
  html_document:
    code_folding: show
---

```{r loadnimble, include=FALSE}
library(nimble)
```

```{r chunksetup, include=FALSE} 
# Following code is only needed for slide generation, not for using R code separately.
library(methods)
read_chunk('chunks.R')
```

# What is MCEM?

MCEM is *Monte Carlo Expectation Maximization*. Many of you may be familiar with the EM (Expectation-Maximization) algorithm. EM is an algorithm that allows one to maximize a likelihood in the presence of missing data, integrating over the missing data.

It maximizes $L(\theta|y) = f(y|\theta) = \int f(y,z|\theta)dz$ where $z$ is missing data. The basic algorithm is to iterate over:

 - E step: Compute $Q(\theta | \theta_t) = E(\log L(\theta|y,z) | y,\theta_t)$
 - M step: Maximize $Q(\theta | \theta_t)$ with respect to $\theta$ to compute $\theta_{t+1}$

until convergence. 

The nice thing is that the notion of missing data extends to latent states, so one can use EM for hierarchical models to maximize the marginal likelihood of the model, thereby finding MLEs for hyperparameters having integrated over the latent states. 

In certain problems, the E step can be done analytically, in closed form, but in many, one cannot easily do the integral. Instead, one can use an MCMC to approximate the expectation. Note that this MCMC is an MCMC over the latent states, keeping the hyperparameters fixed at $\theta_t$, so the MCMC is often an easier MCMC than doing a full MCMC over the entire model. 

# Why is MCEM a nice algorithm for NIMBLE?

MCEM is used some, but not all the extensively, likely for several reasons. First, like EM, it can converge slowly, and in addition, having to use MCMC introduces additional computations and complexity in ensuring and determining convergence. Second, coding it requires coding both MCMC and optimization. 

NIMBLE solves the second problem quite nicely, and implements an established approach to determining convergence so addresses part of the first problem.  We believe it is the first model-generic implementation of MCEM. 

In particular, all a user need to do is provide the model and tell us which are the latent states over which to integrate (but NIMBLE can figure that out too...). 

MCEM is an example of one goal of NIMBLE, which is to enable modular algorithms. We think of modular algorithms as algorithms that borrow components from various algorithms. Since we already had an MCMC engine, it was fairly easy to build a generic MCEM algorithm on top of that. 

# The pump model

We set up the pump model as usual.

```{r, pump-code}
```
```{r, pump-model}
```


# MCEM on the pump model

Here's how easy it is in NIMBLE. Note that we determine the latent nodes in a model-generic way. 

```{r, prep, echo=FALSE}
# so attendees can run code below this without using code from other modules
if(!exists('pump')) source('chunks.R')
```                   

```{r, mcem, eval=FALSE}
mcem = buildMCEM(pump, pump$getNodeNames(latentOnly = TRUE, stochOnly = TRUE),
                 boxConstraints = list(list('alpha', c(0, 100)),
                                       list('beta', c(0, 100))))
output <- mcem()
```

Here's the result. 

```
|-------------|-------------|-------------|-------------|
|-------------------------------------------------------|
Iteration Number: 1.
Current number of MCMC iterations: 1000.
Parameter Estimates: 
    alpha      beta 
0.8188408 1.1320542 
Convergence Criterion: 1.001.
|-------------|-------------|-------------|-------------|
|-------------------------------------------------------|
Iteration Number: 2.
Current number of MCMC iterations: 1000.
Parameter Estimates: 
    alpha      beta 
0.8041004 1.0848407 
Convergence Criterion: 0.005272755.
|-------------|-------------|-------------|-------------|
|-------------------------------------------------------|
Iteration Number: 3.
Current number of MCMC iterations: 1000.
Parameter Estimates: 
    alpha      beta 
0.8111566 1.2155763 
Convergence Criterion: 0.03279659.
|-------------|-------------|-------------|-------------|
|-------------------------------------------------------|
Monte Carlo error too big: increasing MCMC sample size.
|-------------|-------------|-------------|-------------|
|-------------------------------------------------------|
Monte Carlo error too big: increasing MCMC sample size.
|-------------|-------------|-------------|-------------|
|-------------------------------------------------------|
Monte Carlo error too big: increasing MCMC sample size.
|-------------|-------------|-------------|-------------|
|-------------------------------------------------------|
Iteration Number: 4.
Current number of MCMC iterations: 2188.
Parameter Estimates: 
    alpha      beta 
0.8120085 1.2291558 
Convergence Criterion: 0.0008324096.
```

You can compare that to the estimates from running MCMC on the pump model. One possible use for MCEM here would be to provide starting values for MCMC. Or you could fix the hyperparameters and just use MCMC to get posterior draws of the latent states if those are of primary interest. 

In this simple problem, we know that the estimates above are off by about 0.01 for alpha and about 0.025 for beta. In a real application, we would probably want to tighten the convergence criteria. With tightened convergence criteria (not shown), it ends up requiring a fairly large number of iterations and long MCMC chains, so it takes about 20 minutes for what is a pretty simple model, but given how easy it is to set up, in some circumstances that might not be a big drawback. 
