---
title: "Running MCEM for likelihood maximization"
subtitle: "NIMBLE training materials module"
author: "NIMBLE Development Team"
output:
  html_document:
    code_folding: show
---

```{r loadnimble, include=FALSE}
library(nimble)
```

```{r chunksetup, include=FALSE} 
# Following code is only needed for slide generation, not for using R code separately.
library(methods)
read_chunk('chunks.R')
```

# What is MCEM?

MCEM is *Monte Carlo Expectation Maximization*. Many of you may be familiar with the EM (Expectation-Maximization) algorithm. EM is an algorithm that allows one to maximize a likelihood in the presence of missing data, integrating over the missing data.

It maximizes $L(\theta|y) = f(y|\theta) = \int f(y,z|\theta)dz$ where $z$ is missing data. The basic algorithm is to iterate over:

 - E step: Compute $Q(\theta | \theta_t) = E(\log L(\theta|y,z) | y,\theta_t)$
 - M step: Maximize $Q(\theta | \theta_t)$ with respect to $\theta$ to compute $\theta_{t+1}$

until convergence. 

The nice thing is that the notion of missing data extends to latent states, so one can use EM for hierarchical models to maximize the marginal likelihood of the model, thereby finding MLEs for hyperparameters having integrated over the latent states. 

In certain problems, the E step can be done analytically, in closed form, but in many, one cannot easily do the integral. Instead, one can use an MCMC to approximate the expectation. Note that this MCMC is an MCMC over the latent states, keeping the hyperparameters fixed at $\theta_t$, so the MCMC is often an easier MCMC than doing a full MCMC over the entire model. 

# Why is MCEM a nice algorithm for NIMBLE?

MCEM is used some, but not all the extensively, likely for several reasons. First, like EM, it can converge slowly, and in addition, having to use MCMC introduces additional computations and complexity in ensuring and determining convergence. Second, coding it requires coding both MCMC and optimization. 

NIMBLE solves the second problem quite nicely, and implements an established approach to determining convergence so addresses part of the first problem.  We believe it is the first model-generic implementation of MCEM. 

In particular, all a user need to do is provide the model and tell us which are the latent states over which to integrate (but NIMBLE can figure that out too...). 

MCEM is an example of one goal of NIMBLE, which is to enable modular algorithms. We think of modular algorithms as algorithms that borrow components from various algorithms. Since we already had an MCMC engine, it was fairly easy to build a generic MCEM algorithm on top of that. 

# The pump model

We set up the pump model as usual.

```{r, pump-code}
```
```{r, pump-model}
```


# MCEM on the pump model

Here's how easy it is in NIMBLE. Note that we determine the latent nodes in a model-generic way. 

Also note that to avoid numerical issues we did need to constrain the parameter space. 

```{r, prep, echo=FALSE}
# so attendees can run code below this without using code from other modules
if(!exists('pump')) source('chunks.R')
```                   

```{r, mcem, eval=FALSE}
mcem = buildMCEM(pump, pump$getNodeNames(latentOnly = TRUE, stochOnly = TRUE),
                 boxConstraints = list(list('alpha', c(0, 100)),
                                       list('beta', c(0, 100))))
output <- mcem()
```

Here's the result. It ends up requiring a fairly large number of iterations and long MCMC chains, so it takes about 20 minutes for what is a pretty simple model, but given how easy it is to set up, in some circumstances that might not be a big drawback. 

```
Iteration Number: 1.
Current number of MCMC iterations: 1000.
Parameter Estimates: 
    alpha      beta 
0.8160911 1.1304447 
Convergence Criterion: 1.001.
Monte Carlo error too big: increasing MCMC sample size.
Monte Carlo error too big: increasing MCMC sample size.
Monte Carlo error too big: increasing MCMC sample size.
Iteration Number: 2.
Current number of MCMC iterations: 2188.
Parameter Estimates: 
    alpha      beta 
0.8207239 1.1982456 
Convergence Criterion: 0.01686425.
Iteration Number: 3.
Current number of MCMC iterations: 2188.
Parameter Estimates: 
    alpha      beta 
0.8167346 1.2295619 
Convergence Criterion: 0.005174657.
Monte Carlo error too big: increasing MCMC sample size.
Monte Carlo error too big: increasing MCMC sample size.
Monte Carlo error too big: increasing MCMC sample size.
Monte Carlo error too big: increasing MCMC sample size.
Iteration Number: 4.
Current number of MCMC iterations: 9046.
Parameter Estimates: 
    alpha      beta 
0.8194561 1.2504181 
Convergence Criterion: 0.001349063.
Monte Carlo error too big: increasing MCMC sample size.
Monte Carlo error too big: increasing MCMC sample size.
Monte Carlo error too big: increasing MCMC sample size.
Monte Carlo error too big: increasing MCMC sample size.
Monte Carlo error too big: increasing MCMC sample size.
Monte Carlo error too big: increasing MCMC sample size.
Iteration Number: 5.
Current number of MCMC iterations: 97849.
Parameter Estimates: 
   alpha     beta 
0.821782 1.257252 
Convergence Criterion: 9.446297e-05.
```

You can compare that to the estimates from running MCMC on the pump model. One possible use for MCEM here would be to provide starting values for MCMC. Or you could fix the hyperparameters and just use MCMC to get posterior draws of the latent states if those are of primary interest. 
