Extended example of MCMC customization
==========================
NIMBLE training materials module
---------------------------------
Nimble Development Team

```{r chunksetup, include=FALSE} 
# include any code here you don't want to show up in the document,
# e.g. package and dataset loading
if(!('modules' %in% unlist(strsplit(getwd(), split = '/')))) setwd('modules')
library(methods)  # otherwise new() not being found - weird
library(nimble)
read_chunk('module_MCMC_litters.R')
```

# Introduction

Here we'll work through an extended example of customizing an MCMC (and considering maximum likelihood via MCEM) for a tricky, but simple, example model.

The example is the litters example from the original BUGS examples, available with Nimble in the *classic-bugs/vol1/litters* directory of the installed package. 

# Litters model

Here's the original litters model. There are two $G=2$ groups of rat litters, with $N=16$  litters (i.e., mothers) in each group, and a variable number of pups in each litter. Survival of the pups in a litter is governed by a survival probability for each litter, but the probabilities for the litters within a group are considered to come from a common distribution, thereby borrowing strength across the litters in a group.

Here's the model.

```{r}
code <- nimbleCode({
  for (i in 1:G) {
     for (j in 1:N) {
        r[i,j] ~ dbin(p[i,j], n[i,j]);
        p[i,j] ~ dbeta(a[i], b[i]) 
     }
     a[i] ~ dgamma(1, .001)
     b[i] ~ dgamma(1, .001)
   }
})
```

# Setup the model

```
consts <- list(G = 2,N = 16, n = matrix(c(13, 12, 12, 11, 9, 10, 9, 9, 8, 11, 8, 10, 13, 10, 12, 9, 10, 9, 10, 5, 9, 9, 13, 7, 5, 10, 7, 6, 10, 10, 10, 7), nrow = 2))
data = list(r = matrix(c(13, 12, 12, 11, 9, 10, 9, 9, 8, 10, 8, 9, 12, 9, 11, 8, 9, 8, 9, 4, 8, 7, 11, 4, 4, 5, 5, 3, 7, 3, 7, 0), nrow = 2))
inits <- list( a = c(2, 2), b=c(2, 2) )
model <- nimbleModel(code, constants = consts, data = data, inits = inits)
```

# Basic MCMC

Here's Nimble's default MCMC.

```{r, default}
conf <- configureMCMC(model)
conf$addMonitors(c('a', 'b'), thin = 10)
# thinning _only_ to reduce time of plotting (thinning is proven to always reduce efficiency in parameter estimation)
mcmc <- buildMCMC(conf)

Cmodel <- compileNimble(model)
Cmcmc <- compileNimble(mcmc, project = model)

niter <- 10000
set.seed(0)
system.time(Cmcmc$run(niter))

smp_basic <- as.matrix(Cmcmc$mvSamples)

tsplot <- function(x, ...) plot(seq_along(x), x, type = 'l', ...)

par(mfrow = c(3, 4))
tsplot(smp_basic[ , 'a[1]'], main = expression(a[1]), xlab = '', ylab = 'default')
tsplot(smp_basic[ , 'b[1]'], main = expression(b[1]), xlab = '', ylab = '')
tsplot(smp_basic[ , 'a[2]'], main = expression(a[2]), xlab = '', ylab = '', ylim = c(0,20))
tsplot(smp_basic[ , 'b[2]'], main = expression(b[2]), xlab = '', ylab = '', ylim = c(0,6))
```


# Blocking the hyperparameters

Given the posterior correlation between the hyperparameters for each group, an obvious first step is to try a block sampler on $(a_1,b_1)$ and $(a_2, b_2)$.

```{r, hyp-block}
conf$removeSamplers(c('a', 'b'))
conf$addSampler(c('a[1]', 'b[1]'), 'RW_block')
conf$addSampler(c('a[2]', 'b[2]'), 'RW_block')

mcmc <- buildMCMC(conf)
Cmcmc <- compileNimble(mcmc, project = model, resetFunctions = TRUE)

niter <- 10000
set.seed(0)
system.time(Cmcmc$run(niter))

smp_hyp_block <- as.matrix(Cmcmc$mvSamples)

tsplot(smp_hyp_block[ , 'a[1]'], main = expression(a[1]), xlab = '', ylab = 'default')
tsplot(smp_hyp_block[ , 'b[1]'], main = expression(b[1]), xlab = '', ylab = '')
tsplot(smp_hyp_block[ , 'a[2]'], main = expression(a[2]), xlab = '', ylab = '', ylim = c(0,20))
tsplot(smp_hyp_block[ , 'b[2]'], main = expression(b[2]), xlab = '', ylab = '', ylim = c(0,6))
```

# Blocking the random effects

While simply blocking the random effects helped, we might also consider blocking the 


But should we have blocked the hyperparameter with their random effects? Even in simple models, there is a combinatorial explosion of possible blocks.

# Automated blocking

The Nimble developers have developed a procedure for automated blocking in MCMC, based on considering the posterior correlation of all parameters in initial trial runs of the MCMC, as discussed [here](http://arxiv.org/abs/1503.05621).

Let's try automated blocking for this model.

```{r, autoBlock}
auto <- autoBlock(model, autoIt = 10000, run = list())
confAuto <- auto$spec
mcmc <- buildMCMC(confAuto)
```

Given how similar the blocking is to simply blocking the hyperparameters, we won't bother trying it here, but it's easy to do since autoBlock() returns an MCMC configuration argument that can be fed into buildMCMC.




# Integrating over the random effects: user-defined distribution

# Implicitly integrating over the random effects: cross-level sampler

# Using MCEM 

We might also be interested in an empirical Bayes (maximum likelihood for the hyperparameters) analysis. This can be done by Monte Carlo Expectation Maximization, an algorithm that integrates over the random effects numerically using an MCMC sampler for the random effects embedded within an optimization over the hyperparameters. 

```{r, mcem}
model2 <- model$newModel()   # currently an issue with running MCEM on the same model as an MCMC
box = list( list(c('a','b'), c(0, Inf)))
mcem <- buildMCEM(model2, latentNodes = 'p', burnIn = 500, boxConstraints = box)
out <- mcem(maxit = 30) 
```

The MCEM takes a while - basically slow convergence in the optimization is caused by the same issue as the strong dependence in the posterior.

One option would be to reparameterize the model from the $a,b$ parameterization to the mean and scale of the beta distribution. That's easy in Nimble!

# MCEM with reparameterization 

```
code <- nimbleCode({
  for (i in 1:G) {
     for (j in 1:N) {
        r[i,j] ~ dbin(p[i,j], n[i,j]);
        p[i,j] ~ dbeta(mean = mn[i], sd = sig[i]) 
     }
     mn[i] ~ dunif(0, 1)
     sd[i] ~ dunif(0, 1)
   }
})
inits <- list( mn = rep(.5, 2), sd = rep(.5, 2))
model <- nimbleModel(code, constants = consts, data = data, inits = inits)
box = list( list(c('mn','sig'), c(0, 1)))  # check on this - what is good for sig?
mcem <- buildMCEM(model2, latentNodes = 'p', burnIn = 500, boxConstraints = box)
out <- mcem(maxit = 30) 
```
