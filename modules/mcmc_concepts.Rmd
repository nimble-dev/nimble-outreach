---
title: "What is MCMC?"
subtitle: "NIMBLE training materials module"
author: "NIMBLE Development Team"
output:
  html_document:
    code_folding: show
---

```{r chunksetup, include=FALSE} 
# include any code here you don't want to show up in the document,
# e.g. package and dataset loading
if(!('modules' %in% unlist(strsplit(getwd(), split = '/')))) setwd('modules')
library(methods)  # otherwise new() not being found - weird
library(nimble)
```

# Markov chain Monte Carlo (MCMC)

MCMC is one of the main algorithms used for fitting Bayesian models, and is also used as a component in other algorithms, including some that are not Bayesian, such as MCEM.

 - An MCMC sets up a Markov chain on the unknown parameters of a hierarchical model that one runs for many iterations. 
   * If properly constructed the values of the chain represent (correlated) draws from the posterior distribution of the model. 
- The draws can then be used as an approximation to the posterior. 

# How MCMC can go wrong

While MCMC is powerful, it is also dangerous to use without understanding something about it. 

 - First, one often starts the chain using initial values that are not representative of the posterior. In this case, it can take many iterations before the samples from the chain are representative of the distribution. These initial *burn-in* or *warm-up* samples need to be discarded.
 - Second, even once one is past the burn-in phase, and particularly in models where there is a lot of dependence amongst parameters in the posterior distribution, the chain can take a long time to give enough samples to effectly represent the full posterior distribution. When this happens the chain is said to be poorly mixed or slowly mixing, and one may need to run the chain for many iterations (sometimes millions of iterations).

In general an iteration of the chain sequentially samples the different parameters in the model, sometimes with some of the parameters sampled jointly as a block. There are different sub-algorithms, or samplers, that can be used on any given parameter, and users can choose to try out different samplers to see which give the fastest convergence (i.e., least burn-in) and fastest mixing.

# Some basic MCMC samplers

 - A conjugate (aka *Gibbs*) sampler samples each parameter from its conditional distribution given the current values of all other parameters. 
 - A random walk (aka *Metropolis*) sampler proposes a new value for each parameter based on a 'proposal distribution'. Depending on the posterior density of the model parameters under the new and old parameter values, this proposal may be accepted or the original value may be retained. 
 - A blocked random walk sampler does the same thing but proposes two or more parameter values from a multivariate proposal distribution. This can help to improve MCMC performance when the posterior distribution for two or more parameters is highly correlated. 

# A basic example

We'll use a very basic Poisson regression as an example. Here's the BUGS code and the model building. 

```{r, bivar-pois}
n <- 100
x =rnorm(n, 2, 1)
y = rpois(n, 5)

code <- nimbleCode({
    for(i in 1:n) {
        y[i] ~ dpois(exp(b0 + b1*x[i]))
    }
    b0 ~ dnorm(0, sd = 10)
    b1 ~ dnorm(0, sd = 10)
})

inits <- list(b0 = log(mean(y)), b1 = 0)
model <- nimbleModel(code, data = list(y = y),
                 constants = list(x = x, n = n), inits = inits)
```

# Running a basic MCMC

We'll run an MCMC in NIMBLE; for now ignore the NIMBLE code and concentrate on the plots.

```{r, bivar-mcmc, fig.width=14, fig.height=5}
conf <- configureMCMC(model, control = list(scale = 0.02, adaptInterval = 100), 
     print = TRUE)
mcmc <- buildMCMC(conf)
Cmodel <- compileNimble(model)
Cmcmc <- compileNimble(mcmc, project = model)
M <- 1000
set.seed(2)
Cmcmc$run(M)
smp <- as.matrix(Cmcmc$mvSamples)
par(mfrow = c(1,3), cex = 1.4, mai = c(1.2, 1.2, .1, .1),
   mgp = c(2,.7,0))
plot(smp[,1], smp[,2], xlab = 'b0', ylab = 'b1')
plot(1:M, smp[,1], type = 'l', xlab = 'iteration', ylab = 'b0')
plot(1:M, smp[,2], type = 'l', xlab = 'iteration', ylab = 'b1')
```

# Accounting for dependence

This example has the feature that ```b0``` and ```b1``` are highly (negatively) correlated in the posterior (this is a general feature of regression models), so block sampling can help. 

```{r, bivar-mcmc-block, fig.width=14, fig.height=5}
conf$removeSamplers(c('b0','b1'))
conf$addSampler(c('b0','b1'), type = 'RW_block', control =
     list(scale = 0.02, adaptInterval = 100,
          propCov = matrix(c(1,-.8,-.8,1),2)))
mcmc <- buildMCMC(conf)
Cmodel <- compileNimble(model)
Cmcmc <- compileNimble(mcmc, project = model)
Cmodel$setInits(inits)
set.seed(2)
Cmcmc$run(M)
smpBlocked <- as.matrix(Cmcmc$mvSamples)
par(mfrow = c(1,3), cex = 1.4, mai = c(1.2, 1.2, .1, .1),
   mgp = c(2,.7,0))
plot(smpBlocked[,1], smpBlocked[,2], xlab = 'b0', ylab = 'b1')
plot(1:M, smpBlocked[,1], type = 'l', xlab = 'iteration', ylab = 'b0')
plot(1:M, smpBlocked[,2], type = 'l', xlab = 'iteration', ylab = 'b1')
```

That chain hasn't run long enough to give a reliable representation of the posterior. 

This one probably has:
```{r, bivar-mcmc-long, fig.width=14, fig.height=5}
M <- 10000
Cmcmc$run(M)
smpBlocked <- as.matrix(Cmcmc$mvSamples)
par(mfrow = c(1,3), cex = 1.4, mai = c(1.2, 1.2, .1, .1),
   mgp = c(2,.7,0))
plot(smpBlocked[,1], smpBlocked[,2], xlab = 'b0', ylab = 'b1')
plot(1:M, smpBlocked[,1], type = 'l', xlab = 'iteration', ylab = 'b0')
plot(1:M, smpBlocked[,2], type = 'l', xlab = 'iteration', ylab = 'b1')
```


# An example of a long burn-in period

In addition to slow mixing, a long burn-in period can be a problem. 

Suppose we start our chain at unreasonable values that are unlikely according to the posterior. 

```{r, mcmc-burnin, fig.width=14, fig.height=5}
conf <- configureMCMC(model, print = TRUE)
mcmc <- buildMCMC(conf)
Cmodel <- compileNimble(model)
Cmcmc <- compileNimble(mcmc, project = model)
M <- 1000
Cmodel$setInits(list(b0 = 40, b1 = -12))
set.seed(0)
Cmcmc$run(M)
smp <- as.matrix(Cmcmc$mvSamples)
par(mfrow = c(1,3), cex = 1.4, mai = c(1.2, 1.2, .1, .1),
   mgp = c(2,.7,0))
plot(smp[,1], smp[,2], xlab = 'b0', ylab = 'b1')
plot(1:M, smp[,1], type = 'l', xlab = 'iteration', ylab = 'b0')
plot(1:M, smp[,2], type = 'l', xlab = 'iteration', ylab = 'b1')
```

In this case it appears we should discard at least the first 200 iterations. 
