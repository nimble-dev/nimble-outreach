---
title: "Compiling parts of R"
subtitle: "NIMBLE training materials module"
author: "NIMBLE Development Team"
output:
  html_document:
    code_folding: show
---


```{r chunksetup, include=FALSE} 
# include any code here you don't want to show up in the document,
# e.g. package and dataset loading
if(!('modules' %in% unlist(strsplit(getwd(), split = '/')))) setwd('modules')
library(methods)  # otherwise new() not being found - weird
library(nimble)
```

# Using *nimbleFunctions* to compile R code 

While nimbleFunctions are primarily designed to write algorithms to be applied to hierarchical models, you can also use nimbleFunctions as a way to compile your R code to fast C++ code without needing to write any C++ code. Note that this is unlike Rcpp, which provides tools for you to more easily write C++ code that can be called from R.

# Uses of this functionality

How might you use this functionality?  Basically, this is useful for math-focused code that can't be easily vectorized. 

Caveats:

  - the NIMBLE compiler can't compile arbitrary R code, only code that is part of the NIMBLE DSL
  - you need to give a bit of information about variable types and dimensions

# A basic demonstration

Suppose we wanted to mimic R's vectorization capabilities. First we'll see a basic example of exponentiating all the elements of a vector.  Doing that via NIMBLE is a bit silly because we can already have R exponentiate a vector and that exponentiation happens in compiled code. 

nimbleFunctions used in this way need a run function, type and dimension information for arguments and a *returnType()* line.

```{r, rcFun1}
nimExp <- nimbleFunction(
       # run-time code for our computation
       run = function(x = double(1)) {
           returnType(double(1))
           n <- length(x)
           # NIMBLE needs help in determining the dimension and size of out
           declare(out, double(1))
           setSize(out, n)
           # core computation
           for( i in 1:n) 
                out[i] <- exp(x[i])
           return(out)
})
```

We need the *declare()* and *setSize()* in this case (but not the next one) because it's hard for the NIMBLE compiler to figure out the dimension/size of *out*.

Actually nimbleFunctions can handle vectorized code using the Eigen linear algebra package, so let's consider a different implementation.

```{r, rcFun2}
nimExp2 <- nimbleFunction(
       run = function(x = double(1)) {
           returnType(double(1))
           out <- exp(x)
           return(out)
})
```

Now let's compare speed versus R.

```{r, compareRcSpeed}
cnimExp <- compileNimble(nimExp)
cnimExp2 <- compileNimble(nimExp2)

x <- rnorm(1e6)
library(rbenchmark)
benchmark(out0 <- exp(x),
               out1 <- cnimExp(x),
               out2 <- cnimExp2(x),
               columns = c('test','replications','elapsed'),
               replications = 10)
```
               
So that's not all that impressive as all we've done is match (well, not quite) the speed of the native R compiled code. 


# A more involved example

Consider probit regression, which is similar to logistic regression. The probability of a binary outcome is given as
$p = P(Y = 1) = \Phi(X\beta)$ where $\Phi()$ is the normal CDF.

The probit model can be rewritten in a latent variable representation that in a Bayesian context can facilitate MCMC computations to fit the model:
$$ 
\begin{array}
Y & = &  I(W > 0) \\
W & \sim  & N(X\beta , 1) \\
\end{array}
$$

Suppose we know $\beta$. In order to determine $p$ we could use Monte Carlo simulation to estimate this integral:
$P(Y = 1) = \int_{-\infty}^0 f(w) dw$.

Now for probit regression, we could just use standard methods to compute normal pdf integrals. But for the multinomial extension we discuss next, we need Monte Carlo simulation.

# Multinomial probit regression

Let $Y$ be a categorical variable, $Y \in \{{1,2,\ldots,K}\}$. Then a multinomial extension of the latent variable probit model is
$$
Y = {arg\ max}_k {W_k}
$$
$$
W_k \sim N(X\beta_k, 1)
$$

where the `arg max` is simply the $k$ that corresponds to the largest $W_k$.

Now to compute $p = ({P(Y=1), P(Y=2), \ldots, P(Y=K)})$ we can again do Monte Carlo simulation. The basic steps are:

   - iterate m = 1, ... , M
      - for k = 1,...,K, sample $W_k$ from its corresponding normal distribution
      - determine the $k$ such that $W_k$ is the max
   - over the $M$ simulations, count the number of times each category had the largest corresponding $W_k$

The proportion of times each category had the  largest $W_k$ is an estimate of the multinomial proportions of interest.

For our example, we want to do this computation for large $M$ (to reduce Monte Carlo error). 

Note that in a real application, we would likely want to do this for multiple observations with an $n$ by $K$ matrix of $\alpha = X \beta$ values, resulting in an $n$ by $K$ matrix of proportions. But here we'll just consider a single $\alpha$.

# R implementation

```{r, probit}
set.seed(0)
M <- 1000000
system.time({
        alphas <- c(-3, -0.5, -0.25, .1, .15, .29, .4, .45)
        K <- length(alphas)
        # generate W_k ~ N(alpha_k, 1)
        rands <- matrix(rnorm(M*K), nrow = K, ncol = M)
        props <- rep(0, K)
        tmp <- alphas + rands # exploit vectorization
        # now tally the results
        id <- apply(tmp, 2, which.max)
        tbl <- table(id)
        props[as.integer(names(tbl))] <- tbl / M
})

mprobit <- nimbleFunction(
         run = function(alphas = double(1), M = double(0)) {
             returnType(double(1))
             K <- length(alphas)
             declare(props, double(1))
             setSize(props, K)
             declare(w, double(1))
             setSize(w, K)
             for(k in 1:K) 
                   props[k] <- 0.0
             for(m in 1:M) {
                   for(k in 1:K) 
                        w[k] <- alphas[k] + rnorm(1) 
                   maxind <- K
                   max <- w[K]
                   for(k in 1:(K-1)) {
                        if(w[k] > max){
                                maxind <- k
                                max <- w[k]          
                        }
                   }
                   props[maxind] <- props[maxind] + 1
             }
             for(k in 1:K) 
                   props[k] <- props[k]/M
             return(props)
         }
)

cmprobit = compileNimble(mprobit)
set.seed(0)
system.time(
props2 <- cmprobit(alphas, M)
)
```

So we get a nice 5-fold speedup, even though all of the R code was vectorized. In the exercises, you can practice with an example of replacing an explicit for loop in R.

