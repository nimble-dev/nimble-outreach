Aspects of how NIMBLE works
==========================
NIMBLE training materials module
---------------------------------
Nimble Development Team

```{r chunksetup, include=FALSE} 
# include any code here you don't want to show up in the document,
# e.g. package and dataset loading
if(!('modules' %in% unlist(strsplit(getwd(), split = '/')))) setwd('modules')
library(methods)  # otherwise new() not being found - weird
library(nimble)
#read_chunk('module_nimbleFeatures.R')
```

# Introduction

While NIMBLE is largely compatible with BUGS as implemented in WinBUGS/openBUGS/JAGS, we implement things differently in some ways. In this module we'll discuss some of the specific ways that NIMBLE builds models.

# Data nodes and constants

In BUGS and JAGS, constants such as the number of observations or groups, fixed covariate values, fixed indices for vectors, and data are all set as 'data'. 

In NIMBLE, we distinguish between true constants, which are quantities that are never changed, and must be provided when the model is defined, from *data* which is a label for the role a node plays in a model.

Nodes marked as data will by default be protected from any functions that would simulate over their values (though it is possible to change this behavior).

```{r, data}
pumpCode <- nimbleCode({ 
  for (i in 1:N){
      theta[i] ~ dgamma(alpha,beta)
      lambda[i] <- theta[i]*t[i]
      x[i] ~ dpois(lambda[i])
  }
  alpha ~ dexp(1.0)
  beta ~ dgamma(0.1,1.0)
})

N <- 10
t <- c(94.3, 15.7, 62.9, 126, 5.24, 31.4, 1.05, 1.05, 2.1, 10.5)
x <- c(5, 1, 5, 14, 3, 19, 1, 1, 4, 22)
pumpConsts <- list(t = t, N = 10)
pumpData <- list(x = x)
pumpInits <- list(alpha = 1, beta = 1,
         theta = rep(0.1, pumpConsts$N))
pump <- nimbleModel(pumpCode, 
          data = pumpData, constants = pumpConsts, inits = pumpInits)

pump$isData('x')
pump$x
pump$simulate('x')
pump$x
pump$resetData()
pump$isData('x')
pump$x
pump$simulate('x')
pump$x
```

For compatibility with BUGS
and JAGS, NIMBLE allows both to be provided in the the constants
argument to nimbleModel(), in which case NIMBLE determines which are
which, based on which variables appear on the left-hand side of BUGS
declarations.


# Data nodes and missing values

Now let's see how missing values can be handled. In the usual Bayesian paradigm, missing data are just considered to be parameters.

```{r, missing}
pumpData2 <- pumpData
pumpData2$x[3] <- NA
pump$setData(pumpData2)
pump$isData('x')
```

If we set up an MCMC, we see that a sampler (an *end* sampler, which just means sampling a node that hangs off the model graph, with no dependents) is assigned to `x[3]`.

```{r, missing-mcmc}
conf <- configureMCMC(pump)
conf$getSamplers()[13]
```

# Constants vs. right-hand side nodes

Regression predictors (i.e., covariates or independent variables) can be treated either as constants or as nodes in the model. The latter allows one to change the values after the model is built. 

When a variable is specified only on the right-hand side of expression(s) in the model, it does not become a node in the model because it is not a parameter or data, but it is a variable in the model that can be assigned new values. 

Let's see this with the pump model.
```{r, rhs-only}
pump$t
pump$getNodeNames()

pumpInits <- list(alpha = 1, beta = 1,
         theta = rep(0.1, pumpConsts$N), t = t)
pump2 <- nimbleModel(pumpCode, constants = list(N = N),
      data = pumpData, inits = pumpInits) 
pump2$getNodeNames()
pump2$t
pump2$t[1] <- 7.3
pump2$t
```


# Indexing

Nimble can be a bit fussy in terms of requiring users to specify sizes of vectors, matrices, and arrays, either directly in the BUGS code or via the *dimensions* argument to *nimbleModel()*.

For simplicity, we'll just consider specifying dimensions explicitly in the BUGS code. Here's a normal-Wishart example; note that we have indices on all non-scalar objects. 

```{r, indices}
code <- nimbleCode({
     for(i in 1:n) {
           y[i, 1:K] ~ dmnorm(mu[1:K], C[1:K, 1:K])
     }
     mu[1:K] ~ dmnorm(mu0[1:K], Cmu[1:K, 1:K])
     C[1:K, 1:K] ~ dwish(C0[1:K, 1:K], df = 1)
})

# Model checking

In part because of the possibility of omitting needed indices in BUGS code and of having dimension and size mismatches, Nimble does fairly aggressive model checking. Note that this can take a bit of time and also that we report cases where models are created but values for nodes of the model are NA. It's perfectly fine to create a model and not provide values for nodes until later (e.g., letting an MCMC sample from the prior to set initial values), but we do warn you if that's the case, as seen here for *y*, *mu0* and *mu* (and therefore for the objects storing their probability density values.

```{r, indices}
code <- nimbleCode({
     for(i in 1:n) {
           y[i, 1:K] ~ dmnorm(mu[1:K], C[1:K, 1:K])
     }
     mu[1:K] ~ dmnorm(mu0[1:K], Cmu[1:K, 1:K])
     C[1:K, 1:K] ~ dwish(C0[1:K, 1:K], df = 5)
})
K <- 4; n <- 10
model <- nimbleModel(code, constants = list(n = n, K = K),
      inits = list(C0 = diag(rep(1,K)), Cmu = diag(rep(1, K)),
      C = diag(rep(1, K))))

# now have a dimension mismatch - mu[1:3] has mean mu0[1:K]
code <- nimbleCode({
     for(i in 1:n) {
           y[i, 1:K] ~ dmnorm(mu[1:K], C[1:K, 1:K])
     }
     mu[1:3] ~ dmnorm(mu0[1:K], Cmu[1:K, 1:K])
     C[1:K, 1:K] ~ dwish(C0[1:K, 1:K], df = 1)
})
model <- nimbleModel(code, constants = list(n = n, K = K),
      inits = list(C0 = diag(rep(1,K)), Cmu = diag(rep(1, K)),
      C = diag(rep(1, K))))

# now we forget to provide indexing
code <- nimbleCode({
     for(i in 1:n) {
           y[i, 1:K] ~ dmnorm(mu, C)
     }
     mu ~ dmnorm(mu0, Cmu)
     C ~ dwish(C0, df = 1)
})
model <- nimbleModel(code, constants = list(n = n, K = K),
      inits = list(C0 = diag(rep(1,K)), Cmu = diag(rep(1, K)),
      C = diag(rep(1, K))))

```

As you can see, sometimes our error messages are less helpful than they could be.

You can turn off the checking with `check=FALSE` in *nimbleModel()*.

# Alternative parameterizations

BUGS uses fixed (*canonical*) parameterizations and requires that distribution arguments be given in order.

NIMBLE allows you to use alternative parameterizations and allows you to give arguments by name in any order. 

Thus instead of needing to specify a normal distribution based on the precision you can use the standard deviation directly. For example if you wanted to follow Gelman (2006) in using a uniform prior on the standard deviation scale for the scale of a normal distribution, in BUGS or JAGS that would look like this:

```{r, prec}
precCode <- nimbleCode({
y ~ dnorm(mu, tau)
tau <- 1/(sigma*sigma)
sigma ~ dunif(0, 10)
})
```

In NIMBLE you can (and should) do this simply as:
```{r, sd}
sdCode <- nimbleCode({
y ~ dnorm(mu, sd = sigma)
sigma ~ dunif(0, 10)
})
```

NIMBLE uses the same default parameterization as the BUGS canonical parameterization if you don't name your parameters. 

Under the hood, NIMBLE takes whatever parameterization is given and modifies the model to use NIMBLE's canonical parameterization, which is the parameterization used for actual calculations with the distribution. In the case of the normal distribution, this is the standard deviation. You can see the default parameterization, alternative parameterizations, and canonical parameterizations in Table 5.2 of the User Manual.  

# Lifted nodes and implications for updating model quantities

One implication of alternative parameterizations is that if you use a parameterization other than the canonical NIMBLE parameterization, NIMBLE will modify the structure of the model and insert a node in the model. We call this a *lifted node*. 

We can see this in the simple normal example.
```{r, precModel}
precCode <- nimbleCode({
y ~ dnorm(mu, tau)
tau ~ dgamma(1, 1)
})
precModel <- nimbleModel(precCode, inits = list(tau = 100, mu = 0))
precModel$getNodeNames()
```

Here the node called *lifted_d1_over_sqrtPtau_cP* is the standard deviation for *y*. That node is a deterministic function of *tau*:

```{r, precModel-details}
precModel$nodeFunctions$lifted_d1_over_sqrt_oPtau_cP$calculate
```

For this reason, it is important that programs written to adapt to
different model structures use NIMBLEâ€™s systems for querying the model
graph. For example, here's incorrect usage that results in a bug.
```{r, lifted-bug}
precModel$tau <- 0.01
precModel$simulate('y')
precModel$y
precModel$lifted_d1_over_sqrt_oPtau_cP
```

Whoops, *y* was simulated based on the value of the lifted node, which was not updated based on the new value of *tau*. That's why *y* is close to zero even though *tau* is very small. 

And here's correct usage:

```{r, lifted-correct}
precModel$tau <- 0.01
precModel$calculate(precModel$getDependencies('tau'))
# precModel$calculate() # works but recalculates entire model
precModel$simulate('y')
precModel$y
precModel$lifted_d1_over_sqrt_oPtau_cP
```

If one skips the calculate step and assumes the nodes are only those that appear in the BUGS code, one may not get correct results.

# Vectorization

For both deterministic and stochastic nodes, there are cases where one might declare an array of scalar nodes or one might declare a single vector node. Which you choose has implications for computational efficiency.


For example, consider the following two examples:
```{r, nonvector}
code <- nimbleCode({
     for(i in 1:n) {
           logmu[i] <- log(mu[i])
           y[i] ~ dnorm(logmu[i], sd = sigma)
           }
})
```

In this example, for each $i$, `y[i]` and `logmu[i]` are scalar nodes. There are separate simulate and calculate functions for each. A disadvantage of this is that for very large $n$, it can take some time to create and compile the model and associated algorithms. An advantage in this case is that presuming the calculations for each $i$ can be done independently in a given algorithm, unnecessary calculations don't need to be done. For example in doing MCMC sampling for `mu[i]` one needs only calculate `log(mu[i])` and the density for `y[i]`.  

Now suppose that one had done this:
```{r, partial-vector}
code <- nimbleCode({
     logmu[1:n] <- log(mu[1:n])
     for(i in 1:n) {
           y[i] ~ dnorm(logmu[i], sd = sigma)
           }
})
```

Now calculation of *logmu* is done for all the elements at once. There are fewer nodes in the model, which can speed building and compiling models and algorithsm. But if one does an MCMC with Metropolis sampling on each `mu[i]` the logarithm is calculated for all of the elements unnecessarily. In contrast, if one has a block sampler on all of the *mu* elements, then it's fine to have the logarithm calculated for all of the elements. 

Finally, consider a fully-vectorized implementation. 

```{r, full-vector}
code <- nimbleCode({
     logmu[1:n] <- log(mu[1:n])
     y[1:n] ~ dmnorm(logmu[1:n], cov = C[1:n, 1:n])
     C[1:n, 1:n] <- sig*sig*I[1:n,1:n]
})
```

Now there are many fewer nodes, speeding up building and compilation. If one has a joint sampler on all of the elements of *mu*, then all calculations are done in a vectorized fashion (using the Eigen linear algebra package under the hood).

In future releases of NIMBLE we anticipate large improvements in handling of models with many nodes, so the benefits of explicitly vectorizing your BUGS code should decrease.

To summarize: using vectorized declarations, including multivariate distributions, is better in the current NIMBLE because it creates fewer nodes and reduces building and compilation times.  Whether you choose to vectorize depends largely on whether you use algorithms that operate on individual elements separately or operate collectively on the entire vector, such as scalar Metropolis samplers versus blocked samplers in MCMC. 


# Exercises

1) vectorization - see perry's example from the examples page
