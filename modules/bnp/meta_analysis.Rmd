---
title: "Bayesian nonparametrics example (1): Meta analysis"
subtitle: "NIMBLE ENAR webinar module"
author: "NIMBLE Development Team"
output:
  html_document:
    code_folding: show
---


```{r chunksetup, include=FALSE} 
# include any code here you don't want to show up in the document,
# e.g. package and dataset loading
library(methods)  # otherwise new() not being found - weird
library(nimble)
```

# Introduction

Here we'll consider a real example, doing a meta-analysis. We'll start with a standard random effects meta analysis and then robustify the analysis using Bayesian nonparametric methods.

Note that Bayesian nonparametrics (BNP) using Dirichlet process and related methods are not yet in the CRAN version of NIMBLE as we are just finalizing the initial functionality.

To use the BNP versions of the meta analysis, you'll need to install NIMBLE from our Github repository:

```{r, install, eval=FALSE}
library(devtools)
install_github('nimble-dev/nimble/packages/nimble', ref = 'BNP')
```

# Meta analysis example

As most of you will probably know, meta analysis seeks to combine results across multiple studies of the same phenomenon to increase power. It's often applied to clinical trials.

The example is of the side effects of a very popular drug for diabetes called Avandia. The question is whether Avandia uses increases the risk of myocardial infarction (heart attack). There are 48 studies (the 49th study in the data file is different in some ways and excluded here), each with treatment and control arms.

```{r, avandia-view}
dat <- read.csv('avandia.csv')
head(dat)
```

Here we'll start with a generalized linear mixed model (GLMM)-based meta analysis. In fact the model is not so different than our example litters model.

# Basic meta analysis of Avandia MIs

```{r, avandia-setup}
dat <- read.csv('avandia.csv')
dat <- dat[-49, ]

x <- dat$controlMI
n <- dat$nControl
y <- dat$avandiaMI
m <- dat$nAvandia

nStudies <- nrow(dat)
data <- list(x = x, y = y)
constants = list(n = n, m = m, nStudies = nStudies)

codeParam <- nimbleCode({
    for(i in 1:nStudies) {
        y[i] ~ dbin(size = m[i], prob = q[i]) # avandia MIs
        x[i] ~ dbin(size = n[i], prob = p[i]) # control MIs
        q[i] <- expit(theta + gamma[i])       # Avandia log-odds
        p[i] <- expit(gamma[i])               # control log-odds
        gamma[i] ~ dnorm(mu, sd = tau)        # study effects
    }
    theta ~ dflat()        # effect of Avandia
    # random effects hyperparameters
    mu ~ dflat()
    tau ~ dunif(0, 100)
})
```

$\theta$ quantifies the difference in risk between the control and treatment arms, while the $\gamma[i]$ quantify study-specific variation using normally-distributed random effects.

# Running the MCMC

Let's run a basic MCMC.

```{r, mcmc, fig.cap='', fig.width=12, fig.height=5}
inits = list(theta = 0, mu = 0, tau = 1, gamma = rnorm(nStudies))

samples <- nimbleMCMC(code = codeParam, data = data, inits = inits,
                      constants = constants, monitors = c("mu", "tau", "theta", "gamma"),
                      thin = 10, niter = 21000, nburnin = 1000, nchains = 1, setSeed = TRUE)
gammaCols <- grep('gamma', colnames(samples))

par(mfrow = c(1, 4))
ts.plot(samples[ , 'theta'], xlab = 'iteration', ylab = expression(theta))
hist(samples[ , 'theta'], xlab = expression(theta), main = 'effect of Avandia')
gammaMn <- colMeans(samples[ , gammaCols])
hist(gammaMn, xlab = 'posterior means of random effects', main = 'random effects distribution')
hist(samples[1000, gammaCols], xlab = 'single draw of random effects',
                   main = 'random effects distribution')
```

What about the normality assumption - estimated distributions seem skewed PLUS these are generated under the normality assumption!

# Bayesian nonparametrics

When people talk about 'Bayesian nonparametrics' (BNP)  they often mean Dirichlet process and related nonparametric models for flexibly specifying distributions. NIMBLE now provides some standard BNP models that we'll see next.

Gaussian proceses are also nonparametric Bayesian methods, and are feasible in NIMBLE based on using multivariate normal finite-dimensional representations.

Avoiding technical details, a Dirichlet process distribution is a *discrete* distribution that induces clustering of draws from the distribution. It is parameterized by a base measure (a base distribution) and a concentration parameter, $\alpha$. At one extreme, the distribution would cluster all observations into a single value, and at the other it would represent draws from the base measure. 

# Chinese restaurant process

The DP has at its core a model for clustering, which is usually called a Chinese restaurant process.

Here's the idea - we represent the probability of a new customer sitting at each table as follows:

<center><img src="crp.png"></center>

Under the CRP, the probability that the i'th customer sits at an unoccupied table is:

$$ \frac{\alpha}{i-1+\alpha} $$

and the probability the customer sits at table $k$ (where there are $n_k$ people already at the table) is:

$$ \frac{n_k}{i-1+\alpha} $$

# Dirichlet process mixture models

The discreteness of the DP/CRP is good for clustering but bad for representing continuous distributions (like what we would want for the meta analysis).

Instead, we use the DP combined with a standard mixture modeling approach, such as a mixture of normal distributions. The CRP clusters observations (in our case random effects) to mixture components.

# DP-based random effects modeling for meta analysis

```{r, meta-bnp}
codeBNP <- nimbleCode({
    for(i in 1:nStudies) {
        y[i] ~ dbin(size = m[i], prob = q[i]) # avandia MIs
        x[i] ~ dbin(size = n[i], prob = p[i]) # control MIs
        q[i] <- expit(theta + gamma[i])       # Avandia log-odds
        p[i] <- expit(gamma[i])               # control log-odds
        gamma[i] ~ dnorm(mu[i], var = tau[i]) # random effects (from mixture)
        mu[i] <- muTilde[xi[i]]               # mean for component assigned to i'th study
        tau[i] <- tauTilde[xi[i]]             # variance for component assigned to i'th study
    }
    # mixture component parameters drawn from base measures
    for(i in 1:nStudies) {
        muTilde[i] ~ dnorm(mu0, sd = sd0)
        tauTilde[i] ~ dinvgamma(a0, b0)
    }
    # CRP for clustering studies to mixture components
    xi[1:nStudies] ~ dCRP(conc, size = nStudies)
    # hyperparameters
    conc ~ dgamma(1, 1)      # 'alpha' in the CRP discussion
    mu0 ~ dflat()
    sd0 ~ dunif(0, 100)
    a0 ~ dunif(0, 100)
    b0 ~ dunif(0, 100)
    theta ~ dflat()          # effect of Avandia
})
```

The specification is a bit complicated, but just think of it as a nonparametric extension to a mixture of normal distributions as the random effects distribution for $\gamma[i]$, but where we don't fix the maximum number of components.

# Running an MCMC for the DP-based meta analysis

```{r, DP-MCMC, fig.cap='', fig.width=12, fig.height=5}
inits <- list(gamma = rnorm(nStudies), xi = sample(1:2, nStudies, replace = TRUE),
              conc = 1, mu0 = 0, sd0 = 1, a0 = 1, b0 = 1, theta = 0,
              muTilde = rnorm(nStudies), tauTilde = rep(1, nStudies))

samplesBNP <- nimbleMCMC(code = codeBNP, data = data, inits = inits,
               constants = constants,
               monitors = c("theta", "gamma", "conc", "xi", "mu0", "sd0", "a0", "b0"),
               thin = 10, niter = 21000, nburnin = 1000, nchains = 1, setSeed = TRUE)

gammaCols <- grep('gamma', colnames(samplesBNP))
xiCols <- grep('xi', colnames(samplesBNP))

par(mfrow = c(1,5))
ts.plot(samplesBNP[ , 'theta'], xlab = 'iteration', ylab = expression(theta))
hist(samplesBNP[ , 'theta'], xlab = expression(theta), main = 'effect of Avandia')
gammaMn <- colMeans(samplesBNP[ , gammaCols])
hist(gammaMn, xlab = 'posterior means of random effects',
              main = 'random effects distribution')
hist(samplesBNP[1000, gammaCols], xlab = 'single draw of random effects',
                   main = 'random effects distribution')

# How many mixture components are inferred?
xiRes <- samplesBNP[ , xiCols]
nGrps <- apply(xiRes, 1, function(x) length(unique(x)))
ts.plot(nGrps, xlab = 'iteration', ylab = 'number of components')
```

Conclusions: the primary inference seems robust, and there's also not much evidence of multiple components.

What samplers are being used? `nimbleMCMC` doesn't tell us, but we could configure the default MCMC to see:

```{r, DP-samplers}
model <- nimbleModel(codeBNP, constants = constants, data = data, inits = inits)
conf = configureMCMC(model, print = TRUE)
```
